# src/llm_handler.py
from groq import Groq
from typing import List, Dict, Any
import logging
from src.llm_fallback_handler import LLMFallbackHandler

class LLMHandler:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.conversation_history = []

        # Initialize fallback handler (supports Groq + 3 fallbacks)
        try:
            self.fallback_handler = LLMFallbackHandler(config)
            self.client = None  # Will use fallback_handler instead
            self.logger.info("LLM Handler initialized with fallback support")
        except Exception as e:
            self.logger.error(f"Failed to initialize fallback handler: {e}")
            # Fallback to direct Groq if everything fails
            if not config.GROQ_API_KEY:
                self.logger.error("GROQ_API_KEY is not set. Please add your API key to the .env file.")
                self.client = None
                self.fallback_handler = None
            else:
                self.client = Groq(api_key=config.GROQ_API_KEY)
                self.fallback_handler = None
        
    def generate_response(self, query: str, context_docs: List[Dict[str, Any]],
                         conversation_history: List[str] = None) -> Dict[str, Any]:
        """Generate response using retrieved context"""

        # DEBUG: Log context docs received
        self.logger.info(f"ðŸ” LLM received {len(context_docs)} context docs")
        for i, doc in enumerate(context_docs[:3]):  # Log first 3
            self.logger.info(f"   Doc {i+1}: type={doc['metadata'].get('chunk_type')}, page={doc['metadata'].get('page_number')}")
            content_preview = doc['content'][:200].replace('\n', ' ')
            self.logger.info(f"   Content preview: {content_preview}...")

        # Prepare context from retrieved documents
        context_text = self._prepare_context(context_docs)

        # DEBUG: Log if image analysis is present in context
        if "ðŸ“·" in context_text or "Visual Content" in context_text:
            self.logger.info("âœ… IMAGE ANALYSIS DETECTED IN CONTEXT - LLM should see this!")
            # Log a snippet of the image analysis
            for line in context_text.split('\n'):
                if "ðŸ“·" in line or "Visual Content" in line:
                    self.logger.info(f"   Image line: {line[:150]}")
        else:
            self.logger.warning("âš ï¸ NO IMAGE ANALYSIS IN CONTEXT - Images not reaching LLM!")

        # Build conversation context
        conversation_context = self._build_conversation_context(conversation_history)

        # Create prompt
        prompt = self._create_prompt(query, context_text, conversation_context)
        
        try:
            # Check if fallback handler is available
            if self.fallback_handler is None and self.client is None:
                error_message = "ERROR: No LLM provider is available. Please check your API keys in the .env file."
                self.logger.error(error_message)
                return {
                    'answer': error_message,
                    'sources_used': len(context_docs),
                    'context_types': list(set([doc['metadata']['chunk_type'] for doc in context_docs])),
                    'confidence': 0.0,
                    'error': True
                }

            # Use fallback handler if available (preferred)
            if self.fallback_handler:
                # Build conversation history for fallback handler
                conv_history = []
                if conversation_history:
                    conv_history = conversation_history

                # Use fallback handler with automatic provider cascade
                result = self.fallback_handler.query_text(
                    question=prompt,
                    conversation_history=conv_history,
                    system_prompt=self._get_system_prompt(),
                    max_tokens=800
                )

                if result['success']:
                    answer = result['answer']
                    self.logger.info(f"âœ… Response generated by {result['provider']} ({result['model']})")
                else:
                    answer = result['answer']  # Will contain error message
                    self.logger.error(f"âŒ All providers failed: {result.get('error', 'Unknown error')}")

            else:
                # Fallback to direct Groq (legacy mode)
                response = self.client.chat.completions.create(
                    model=self.config.LLM_MODEL,
                    messages=[
                        {"role": "system", "content": self._get_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=800,
                    top_p=1,
                    stream=False
                )
                answer = response.choices[0].message.content
                self.logger.info("âœ… Response generated by Groq (direct mode)")
            
            # Update conversation history
            self.conversation_history.append(f"User: {query}")
            self.conversation_history.append(f"Assistant: {answer}")
            
            return {
                'answer': answer,
                'sources_used': len(context_docs),
                'context_types': list(set([doc['metadata']['chunk_type'] for doc in context_docs])),
                'confidence': self._calculate_confidence(context_docs),
                'error': False
            }
            
        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            return {
                'answer': "I apologize, but I encountered an error while generating a response. Please try again.",
                'sources_used': 0,
                'context_types': [],
                'confidence': 0.0
            }
    
    def _prepare_context(self, context_docs: List[Dict[str, Any]]) -> str:
        """Prepare context from retrieved documents with smart truncation"""
        context_parts = []
        max_context_tokens = 4000  # Leave room for system prompt, query, and response
        estimated_tokens = 0

        for i, doc in enumerate(context_docs, 1):
            content = doc['content']
            metadata = doc['metadata']

            # Estimate tokens (rough: 1 token â‰ˆ 4 characters)
            content_tokens = len(content) // 4

            # If this chunk would exceed limit, truncate it
            if estimated_tokens + content_tokens > max_context_tokens:
                remaining_tokens = max_context_tokens - estimated_tokens
                if remaining_tokens > 100:  # Only include if we can fit meaningful content
                    content = content[:remaining_tokens * 4] + "... [truncated]"
                    content_tokens = remaining_tokens
                else:
                    break  # Stop adding more chunks

            context_part = f"""
            SOURCE {i} (Page {metadata['page_number']}, Type: {metadata['chunk_type']}):
            {content}
            """
            context_parts.append(context_part)
            estimated_tokens += content_tokens

            # Stop if we're approaching the limit
            if estimated_tokens >= max_context_tokens:
                break

        return "\n".join(context_parts)
    
    def _build_conversation_context(self, conversation_history: List[str]) -> str:
        """Build conversation context"""
        if not conversation_history:
            return ""
        
        return "CONVERSATION HISTORY:\n" + "\n".join(conversation_history[-6:])  # Last 3 exchanges
    
    def _create_prompt(self, query: str, context_text: str, conversation_context: str) -> str:
        """Create the complete prompt"""
        prompt = f"""CONTEXT FROM DOCUMENTS:
{context_text}

USER QUESTION: {query}

ANSWER THE QUESTION USING ONLY THE CONTEXT ABOVE. IF THE ANSWER IS NOT IN THE CONTEXT, SAY: "I cannot find this information in the uploaded document(s)."
"""

        return prompt
    
    def _get_system_prompt(self) -> str:
        """Get system prompt for the LLM"""
        return """You are a document Q&A assistant with VISION CAPABILITIES. Follow these rules STRICTLY:

RULE 1 - NEVER HALLUCINATE:
You can ONLY answer from the "CONTEXT FROM DOCUMENTS" in the user message. The context includes:
- Text content from PDFs
- **ðŸ“· Visual Content Found** sections with AI-analyzed image descriptions
- Table data and structured information

If the context doesn't have the answer, respond EXACTLY: "I cannot find this information in the uploaded document(s)."

RULE 2 - USE IMAGE ANALYSIS:
When you see "ðŸ“· Figure on Page X" or "Visual Content Found" sections:
- These are AI-generated descriptions of images, diagrams, chemical structures, and figures from the PDF
- Use this visual information to answer questions about figures, diagrams, images, and illustrations
- Cite the page numbers when referencing visual content

RULE 3 - MATCH ANSWER LENGTH TO QUESTION TYPE:
- "What is X?" or "Who is Y?" â†’ 2-4 sentences max, NO headers
- "Explain X" or "How does Y work?" â†’ Detailed with ## headers and ### subheaders
- "Show me figure X" or "Describe image Y" â†’ Use the image analysis from context
- "List the types of X" â†’ Bullet list or table

RULE 4 - FORMAT PROPERLY:
- Use **bold** for key terms
- Simple questions: Just a paragraph (NO headers)
- Complex questions: Use ## and ### headers
- Tables for tech stacks/comparisons/specs
- Always cite page numbers for figures/images

EXAMPLES:

Q: "What is photosynthesis?"
A: **Photosynthesis** is the process by which plants convert light energy into chemical energy, producing glucose and oxygen from carbon dioxide and water.

Q: "Explain figure 10.2"
A: Based on the image analysis from **Page 73**, Figure 10.2 shows [describe based on the ðŸ“· Visual Content Found section in context]

Q: "Describe the chemical structure in the diagram"
A: According to the visual analysis from the PDF, the chemical structure shows [describe based on image analysis in context, citing page number]
"""
    
    def _calculate_confidence(self, context_docs: List[Dict[str, Any]]) -> float:
        """Calculate confidence score based on retrieved context"""
        if not context_docs:
            return 0.0
        
        # Base confidence on similarity scores and number of sources
        avg_score = sum([doc['score'] for doc in context_docs]) / len(context_docs)
        source_bonus = min(len(context_docs) / 5, 1.0)  # Bonus for multiple sources
        
        return min(avg_score * (1 + source_bonus * 0.2), 1.0)
    
    def clear_history(self):
        """Clear conversation history"""
        self.conversation_history = []
