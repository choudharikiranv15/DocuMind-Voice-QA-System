# src/llm_handler.py
from groq import Groq
from typing import List, Dict, Any
import logging
from src.llm_fallback_handler import LLMFallbackHandler

class LLMHandler:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.conversation_history = []

        # Initialize fallback handler (supports Groq + 3 fallbacks)
        try:
            self.fallback_handler = LLMFallbackHandler(config)
            self.client = None  # Will use fallback_handler instead
            self.logger.info("LLM Handler initialized with fallback support")
        except Exception as e:
            self.logger.error(f"Failed to initialize fallback handler: {e}")
            # Fallback to direct Groq if everything fails
            if not config.GROQ_API_KEY:
                self.logger.error("GROQ_API_KEY is not set. Please add your API key to the .env file.")
                self.client = None
                self.fallback_handler = None
            else:
                self.client = Groq(api_key=config.GROQ_API_KEY)
                self.fallback_handler = None
        
    def generate_response(self, query: str, context_docs: List[Dict[str, Any]],
                         conversation_history: List[str] = None) -> Dict[str, Any]:
        """Generate response using retrieved context"""

        # DEBUG: Log context docs received
        self.logger.info(f"ðŸ” LLM received {len(context_docs)} context docs")
        for i, doc in enumerate(context_docs[:3]):  # Log first 3
            self.logger.info(f"   Doc {i+1}: type={doc['metadata'].get('chunk_type')}, page={doc['metadata'].get('page_number')}")
            content_preview = doc['content'][:200].replace('\n', ' ')
            self.logger.info(f"   Content preview: {content_preview}...")

        # Prepare context from retrieved documents
        context_text = self._prepare_context(context_docs)

        # DEBUG: Log if image analysis is present in context
        if "ðŸ“·" in context_text or "Visual Content" in context_text:
            self.logger.info("âœ… IMAGE ANALYSIS DETECTED IN CONTEXT - LLM should see this!")
            # Log a snippet of the image analysis
            for line in context_text.split('\n'):
                if "ðŸ“·" in line or "Visual Content" in line:
                    self.logger.info(f"   Image line: {line[:150]}")
        else:
            self.logger.warning("âš ï¸ NO IMAGE ANALYSIS IN CONTEXT - Images not reaching LLM!")

        # Build conversation context
        conversation_context = self._build_conversation_context(conversation_history)

        # Create prompt
        prompt = self._create_prompt(query, context_text, conversation_context)
        
        try:
            # Check if fallback handler is available
            if self.fallback_handler is None and self.client is None:
                error_message = "ERROR: No LLM provider is available. Please check your API keys in the .env file."
                self.logger.error(error_message)
                return {
                    'answer': error_message,
                    'sources_used': len(context_docs),
                    'context_types': list(set([doc['metadata']['chunk_type'] for doc in context_docs])),
                    'confidence': 0.0,
                    'error': True
                }

            # Use fallback handler if available (preferred)
            if self.fallback_handler:
                # Build conversation history for fallback handler
                conv_history = []
                if conversation_history:
                    conv_history = conversation_history

                # Determine max_tokens based on query type
                if any(keyword in query.lower() for keyword in ['summarize', 'summary', 'explain', 'describe', 'long']):
                    max_tokens = 2000  # Long responses for summaries/explanations
                else:
                    max_tokens = 500  # Shorter for direct questions

                # Use fallback handler with automatic provider cascade
                result = self.fallback_handler.query_text(
                    question=prompt,
                    conversation_history=conv_history,
                    system_prompt=self._get_system_prompt(),
                    max_tokens=max_tokens
                )

                if result['success']:
                    answer = result['answer']
                    self.logger.info(f"âœ… Response generated by {result['provider']} ({result['model']})")
                else:
                    answer = result['answer']  # Will contain error message
                    self.logger.error(f"âŒ All providers failed: {result.get('error', 'Unknown error')}")

            else:
                # Fallback to direct Groq (legacy mode)
                # Determine max_tokens based on query type
                if any(keyword in query.lower() for keyword in ['summarize', 'summary', 'explain', 'describe']):
                    max_tokens = 2000  # Long responses for summaries/explanations
                else:
                    max_tokens = 500  # Shorter for direct questions

                response = self.client.chat.completions.create(
                    model=self.config.LLM_MODEL,
                    messages=[
                        {"role": "system", "content": self._get_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.4,  # Lower for more direct, factual responses
                    max_tokens=max_tokens,
                    top_p=1,
                    stream=False
                )
                answer = response.choices[0].message.content
                self.logger.info("âœ… Response generated by Groq (direct mode)")
            
            # Update conversation history
            self.conversation_history.append(f"User: {query}")
            self.conversation_history.append(f"Assistant: {answer}")
            
            return {
                'answer': answer,
                'sources_used': len(context_docs),
                'context_types': list(set([doc['metadata']['chunk_type'] for doc in context_docs])),
                'confidence': self._calculate_confidence(context_docs),
                'error': False
            }
            
        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            return {
                'answer': "I apologize, but I encountered an error while generating a response. Please try again.",
                'sources_used': 0,
                'context_types': [],
                'confidence': 0.0
            }
    
    def _prepare_context(self, context_docs: List[Dict[str, Any]]) -> str:
        """Prepare context from retrieved documents with smart truncation"""
        context_parts = []
        max_context_tokens = 4000  # Leave room for system prompt, query, and response
        estimated_tokens = 0

        for i, doc in enumerate(context_docs, 1):
            content = doc['content']
            metadata = doc['metadata']

            # Estimate tokens (rough: 1 token â‰ˆ 4 characters)
            content_tokens = len(content) // 4

            # If this chunk would exceed limit, truncate it
            if estimated_tokens + content_tokens > max_context_tokens:
                remaining_tokens = max_context_tokens - estimated_tokens
                if remaining_tokens > 100:  # Only include if we can fit meaningful content
                    content = content[:remaining_tokens * 4] + "... [truncated]"
                    content_tokens = remaining_tokens
                else:
                    break  # Stop adding more chunks

            # Minimal context formatting - just content and page
            context_part = f"""{content} [Page {metadata['page_number']}]

"""
            context_parts.append(context_part)
            estimated_tokens += content_tokens

            # Stop if we're approaching the limit
            if estimated_tokens >= max_context_tokens:
                break

        return "\n".join(context_parts)
    
    def _build_conversation_context(self, conversation_history: List[str]) -> str:
        """Build conversation context"""
        if not conversation_history:
            return ""
        
        return "CONVERSATION HISTORY:\n" + "\n".join(conversation_history[-6:])  # Last 3 exchanges
    
    def _create_prompt(self, query: str, context_text: str, conversation_context: str) -> str:
        """Create the complete prompt"""
        # Detect if question is document-specific or general knowledge
        is_document_specific = self._is_document_specific_query(query)

        if is_document_specific:
            # Strict document-only mode for document-specific questions
            prompt = f"""CONTEXT FROM DOCUMENTS:
{context_text}

USER QUESTION: {query}

ANSWER THE QUESTION USING ONLY THE CONTEXT ABOVE. IF THE ANSWER IS NOT IN THE CONTEXT, SAY: "I cannot find this information in the uploaded document(s)."
"""
        else:
            # General knowledge mode - can use general knowledge but should reference documents if relevant
            prompt = f"""CONTEXT FROM UPLOADED DOCUMENTS (if relevant):
{context_text}

USER QUESTION: {query}

INSTRUCTIONS:
- This is a general knowledge question
- Answer using your general knowledge first
- If the uploaded documents contain relevant information, you may mention it briefly at the end
- Keep your answer concise and focused on the question
"""

        return prompt

    def _is_document_specific_query(self, query: str) -> bool:
        """Detect if query is asking about document content specifically"""
        query_lower = query.lower()

        # Document-specific indicators
        document_indicators = [
            'in the document', 'in my document', 'in this document',
            'according to', 'what does the document say',
            'from the document', 'in the pdf', 'in my pdf',
            'the document mentions', 'document states',
            'extract from', 'find in document'
        ]

        # If any document-specific phrase is found, it's a document query
        if any(indicator in query_lower for indicator in document_indicators):
            return True

        # Otherwise, treat as general knowledge query
        return False
    
    def _get_system_prompt(self) -> str:
        """Get system prompt for the LLM"""
        return """You are a friendly and knowledgeable tutor. Answer questions naturally and directly.

RESPONSE MODES:
1. For GENERAL KNOWLEDGE questions: Use your knowledge to give accurate, helpful answers
2. For DOCUMENT-SPECIFIC questions: Answer strictly from the provided context

STYLE RULES:
1. Give DIRECT answers - no meta-commentary
2. NEVER say "Document Excerpt X" or "it is mentioned that"
3. Answer like you're explaining to a friend
4. For document queries with no answer: "I cannot find this information in the uploaded document(s)."

LENGTH:
- "What is X?" â†’ 2-4 sentences
- "Explain X" â†’ 4-6 sentences maximum
- Keep answers CONCISE and CLEAR

FORBIDDEN PHRASES:
âŒ "In the context above"
âŒ "Document Excerpt X"
âŒ "it is mentioned that"
âŒ "according to the document"
âŒ Any meta-commentary about sources or context

ALLOWED (Simple and Direct):
âœ… Just answer naturally
âœ… Include page numbers in parentheses if helpful: (Page 5)
âœ… Use **bold** for key terms

EXAMPLES:

Q: "What are ketones?"
A: **Ketones** are organic compounds with a carbonyl group (C=O) bonded to two carbon atoms. They're important in organic chemistry and used in flavourings, plastics, and drugs (Page 71).

Q: "What are real numbers?"
A: **Real numbers** include all rational and irrational numbers that can be represented on a number line. Examples include integers like 3, fractions like 1/2, and irrational numbers like âˆš2 and Ï€ (Page 1).

Q: "Why do we use chemistry?"
A: We use chemistry to create medicines for treating diseases, preserve food, develop cleaning products, and make materials like plastics and fabrics that we use daily (Page 165).

REMEMBER: Just answer directly! No analysis of what's in the context!
"""
    
    def _calculate_confidence(self, context_docs: List[Dict[str, Any]]) -> float:
        """Calculate confidence score based on retrieved context"""
        if not context_docs:
            return 0.0
        
        # Base confidence on similarity scores and number of sources
        avg_score = sum([doc['score'] for doc in context_docs]) / len(context_docs)
        source_bonus = min(len(context_docs) / 5, 1.0)  # Bonus for multiple sources
        
        return min(avg_score * (1 + source_bonus * 0.2), 1.0)
    
    def clear_history(self):
        """Clear conversation history"""
        self.conversation_history = []
